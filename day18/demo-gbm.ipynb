{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a\n",
    "machine learning technique for regression and classification problems, which\n",
    "produces a prediction model in the form of an ensemble of weak prediction\n",
    "models, typically decision trees. It builds the model in an iterative fashion\n",
    "like other boosting methods do, and it generalizes them by allowing optimization\n",
    "of an arbitrary differentiable loss function.\n",
    "\n",
    "It is recommended that you read through the accompanying [Classification and\n",
    "Regression Trees Tutorial](decision-trees.ipynb) for an overview of decision\n",
    "trees.\n",
    "\n",
    "## History\n",
    "\n",
    "Boosting is one of the most powerful learning ideas introduced in the last\n",
    "twenty years. It was originally designed for classification problems, but it can\n",
    "be extended to regression as well. The motivation for boosting was a procedure\n",
    "that combines the outputs of many \"weak\" classifiers to produce a powerful\n",
    "\"committee.\"  A weak classifier (e.g. decision tree) is one whose error rate is\n",
    "only slightly better than random guessing.\n",
    "\n",
    "[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) short for \"Adaptive\n",
    "Boosting\", is a machine learning meta-algorithm formulated by [Yoav\n",
    "Freund](https://en.wikipedia.org/wiki/Yoav_Freund) and [Robert\n",
    "Schapire](https://en.wikipedia.org/wiki/Robert_Schapire) in 1996, which is now\n",
    "considered to be a special case of Gradient Boosting.  There are [some\n",
    "differences](http://stats.stackexchange.com/questions/164233/intuitive-\n",
    "explanations-of-differences-between-gradient-boosting-trees-gbm-ad) between the\n",
    "AdaBoost algorithm and modern Gradient Boosting.  In the AdaBoost algorithm, the\n",
    "\"shortcomings\" of existing weak learners are identified by high-weight data\n",
    "points, however in Gradient Boosting, the shortcomings are identified by\n",
    "gradients.\n",
    "\n",
    "The idea of gradient boosting originated in the observation by Leo Breiman that\n",
    "boosting can be interpreted as an optimization algorithm on a suitable cost\n",
    "function. Explicit regression gradient boosting algorithms were subsequently\n",
    "developed by Jerome H. Friedman, simultaneously with the more general functional\n",
    "gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and\n",
    "Marcus Frean. The latter two papers introduced the abstract view of boosting\n",
    "algorithms as iterative functional gradient descent algorithms. That is,\n",
    "algorithms that optimize a cost function over function space by iteratively\n",
    "choosing a function (weak hypothesis) that points in the negative gradient\n",
    "direction. This functional gradient view of boosting has led to the development\n",
    "of boosting algorithms in many areas of machine learning and statistics beyond\n",
    "regression and classification.\n",
    "\n",
    "In general, in terms of model performance, we have the following heirarchy:\n",
    "\n",
    "$$Boosting > Random \\: Forest > Bagging > Single \\: Tree$$\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "[Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a\n",
    "machine learning technique for regression and classification problems, which\n",
    "produces a prediction model in the form of an ensemble of weak prediction\n",
    "models, typically decision trees. It builds the model in a stage-wise fashion\n",
    "like other boosting methods do, and it generalizes them by allowing optimization\n",
    "of an arbitrary differentiable loss function.\n",
    "\n",
    "The purpose of boosting is to sequentially apply the weak classification\n",
    "algorithm to repeatedly modified versions of the data, thereby producing a\n",
    "sequence of weak classifiers $G_m(x)$, $m = 1, 2, ... , M$.\n",
    "\n",
    "## Stagewise Additive Modeling\n",
    "\n",
    "Boosting builds an additive model:\n",
    "\n",
    "$$F(x) = \\sum_{m=1}^M \\beta_m b(x; \\gamma_m)$$\n",
    "\n",
    "where $b(x; \\gamma_m)$ is a tree and $\\gamma_m$ parameterizes the splits.  With\n",
    "boosting, the parameters, $(\\beta_m, \\gamma_m)$ are fit in a *stagewise*\n",
    "fashion.  This slows the process down, and overfits less quickly.\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "- AdaBoost builds an additive logistic regression model by stagewise fitting.\n",
    "- AdaBoost uses an exponential loss function of the form, $L(y, F(x)) =\n",
    "exp(-yF(x))$, similar to the negative binomial log-likelihood loss.\n",
    "- The principal attraction of the exponential loss in the context of additive\n",
    "modeling is computational; it leads to the simple modular reweighting\n",
    "- Instead of fitting trees to residuals, the special form of the exponential\n",
    "loss function in AdaBoost leads to fitting trees to weighted versions of the\n",
    "original data.\n",
    "\n",
    "## Gradient Boosting Algorithm\n",
    "\n",
    "Friedman's Gradient Boosting Algorithm for a generic loss function, $L(y_i,\n",
    "\\gamma)$:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Loss Functions and Gradients\n",
    "\n",
    "\n",
    "\n",
    "The optimal number of iterations, T, and the learning rate, Î», depend on each\n",
    "other.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Stochastic GBM\n",
    "\n",
    "[Stochastic Gradient Boosting](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)\n",
    "(Friedman, 2002) proposed the stochastic gradient boosting algorithm that simply\n",
    "samples uniformly without replacement from the dataset before estimating the\n",
    "next gradient step. He found that this additional step greatly improved\n",
    "performance.\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "- It's more common to grow shorter trees (\"shrubs\" or \"stumps\") in GBM than you\n",
    "do in Random Forest.\n",
    "- It's useful to try a variety of column sample (and column sample per tree)\n",
    "rates.\n",
    "- Don't assume that the set of optimal tuning parameters for one implementation\n",
    "of GBM will carry over and also be optimal in a different GBM implementation.\n",
    "\n",
    "## Resources\n",
    " - [Trevor Hastie - Gradient Boosting & Random Forests at H2O World 2014](https:\n",
    "//www.youtube.com/watch?v=wPqtzj5VZus&index=16&list=PLNtMya54qvOFQhSZ4IKKXRbMkyL\n",
    "Mn0caa) (YouTube)\n",
    " - [Trevor Hastie - Data Science of GBM\n",
    "(2013)](http://www.slideshare.net/0xdata/gbm-27891077) (slides)\n",
    " - [Mark Landry - Gradient Boosting Method and Random Forest at H2O World\n",
    "2015](https://www.youtube.com/watch?v=9wn1f-30_ZY) (YouTube)\n",
    " - [Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at\n",
    "PyData London 2014](https://www.youtube.com/watch?v=IXZKgIsZRm0) (YouTube)\n",
    " - [Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a\n",
    "tutorial](http://journal.frontiersin.org/article/10.3389/fnbot.2013.00021/full)\n",
    "(blog post)<br>\n",
    "[Kaggle Blog) on Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv36",
   "language": "python",
   "name": "itv36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
