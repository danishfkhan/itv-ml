{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invented by Frank Rosenblatt at the Cornell Aeronautical Laboratory in the late 1950s, the development of the perceptron was originally motivated by efforts to simulate the human brain. A brain is composed of cells called neurons that process information, and connections between neurons are called synapses, through which information is transmitted. The human brain has been estimated to be composed of as many as 100 billion neurons and 100 trillion synapses. Illustrated in the following image, the main components of a neuron are dendrites, a body, and an axon. The dendrites receive electrical signals from other neurons. The signals are processed in the neuron's body, which then sends a signal through the axon to another neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An individual neuron can be thought of as a computational unit that processes one or more inputs to produce an output. A perceptron functions analogously to a neuron; it accepts one or more inputs, processes them, and returns an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circles labeled x1, x2, and x3 are input units. Each input unit represents one feature. Perceptrons frequently use an additional input unit that represents a constant bias term, but this input unit is usually omitted from diagrams. The circle in the center is a computational unit, or the neuron's body. The edges connecting the input units to the computational unit are analogous to dendrites. Each edge is associated with a parameter, or weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 1\n",
    "x2 = 0\n",
    "x3 = -1.5\n",
    "w1 = 1/2\n",
    "w2 = 3/5\n",
    "w3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1*x1 + w2*x2 + w3*x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the above equation by using an activation function phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common activation function is unit step function, the Heaviside step function. Another common activation function is the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron rule: Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning algorithm begins by setting the weights to zero, or to small random values. It then predicts the class for a training instance. The perceptron is an error-driven learning algorithm; if the prediction is correct, the algorithm continues to the next instance. If the prediction is incorrect, the algorithm updates the weights. More formally, the update rule is given by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each training instance, the value of the parameter for each feature is incremented by α(dj - yj(t)) xj,i, where dj is the true class for instance j, yj(t) is the predicted class for instance j, xj,i is the value of the ith feature for instance j, and α is a hyperparameter that controls the learning rate. If the prediction is correct, dj - yj(t) equals zero, and the term α(dj - yj(t)) xj,i equals zero. That is, if the prediction is correct, the weight is not updated. If the prediction is incorrect, we compute the product of dj - yj(t), the value of the feature, and the learning rate. We then add the product (which may be negative) to the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w_i = w_i + delta_w_i\n",
    "delta_w_i = eta * (y_i - y_hat_i) * X_i\n",
    "y_hat = (sum(w_i*x_i) >= 0)\n",
    "\n",
    "Here,\n",
    "y_i = truth value for ith observation\n",
    "y_hat_i = predicted value for ith observation\n",
    "eta = learning rate\n",
    "X = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This update rule is similar to the update rule for gradient descent in that the weights are adjusted towards classifying the instance correctly and the size of the update is controlled by a learning rate. Each pass through the training instances is called an epoch. The learning algorithm has converged when it completes an epoch without misclassifying any of the instances. The learning algorithm is not guaranteed to converge; later we will discuss linearly inseparable datasets for which convergence is impossible. For this reason, the learning algorithm also requires a hyperparameter that specifies the maximum number of epochs that can be completed before the algorithm terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Binary classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third epoch\n",
    "\n",
    "![Fig](imgs/img_009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a hyperplane\n",
    "\n",
    "[Hyperplane](https://www.youtube.com/watch?v=xNSR9p9XLt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xNSR9p9XLt8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xNSR9p9XLt8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of a perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron uses a hyperplane to separate the positive and negative classes. A simple example of a classification problem that is linearly inseparable is the logical exclusive disjunction, or XOR. The output of XOR is 1 when one of its inputs is equal to 1 and the other is equal to 0. Otherwise, the output is 0. The inputs and outputs of XOR are plotted in two dimensions in the following graph. When XOR outputs 1, the instance is marked with a circle; when XOR outputs 0, the instance is marked with a diamond:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to separate circles from diamonds using a single straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 techniques that can be used to model data that is not linearly separable. First technique is\n",
    "called kernelization and is used in support vector machines. Second technique is called artificial neual networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network\n",
    "\n",
    "We will discuss ANNs, powerful nonlinear models for supervised and unsupervised tasks that use a different strategy to overcome the perceptron's limitations. If the perceptron is analogous to a neuron, an ANN, or neural net, is analogous to a brain. As billions of neurons with trillions of synapses comprise a human brain, an ANN is a directed graph of artificial neurons. The graph's edges are weighted; these weights are the parameters of the model that must be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Froward and Feed back ANNs\n",
    "\n",
    "There are two main types of ANN. Feed-forward neural networks are the most common type and are defined by their directed acyclic graphs. Information travels in one direction only, towards the output layer, in feed-forward neural networks. Conversely, feedback neural networks, or recurrent neural networks, contain cycles. The feedback cycles can represent an internal state for the network that can cause the network's behavior to change over time based on its input. Feed-forward neural networks are commonly used to learn a function to map an input to an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptrons have three or more layers of artificial neurons that form a directed, acyclic graph. Generally, each layer is fully connected to the subsequent layer; the output, or activation, of each artificial neuron in a layer is an input to every artificial neuron in the next layer. Features are input through the Input layer. The simple neurons in the input layer are connected to at least one Hidden layer. Hidden layers represents latent variables; these cannot be observed in the training data. The hidden neurons in these layers are often called hidden units. Finally, the last hidden layer is connected to an Output layer; the activations of this layer are the predicted values of the response variable. The following diagram depicts the architecture of a multi-layer perceptron with three layers. The neurons labeled +1 are constant bias neurons and are not depicted in most architecture diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0013.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "In this section, we will discuss another method for efficiently estimating the optimal values of the model's parameters called gradient descent. Note that our definition of the goodness-of-fit has not changed; we will still use gradient descent to estimate the values of the model's parameters that minimize the value of the cost function.\n",
    "Gradient descent is sometimes described by the analogy of a blindfolded person who is trying to find her way from somewhere on a mountainside to the lowest point of the valley. The person cannot see the topography, be she can judge the steepness of each step she takes. She takes a step in the direction with the steepest decline. She then takes another step, again in the direction with the steepest decline. The lengths of her strides are proportional to the steepness of the terrain at her current position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She takes big steps when the terrain is steep, as she is confident that she is still near the peak and that she will not overshoot the valley's lowest point. She takes smaller steps as the terrain becomes less steep. If she were to continue taking large steps, she may accidentally step over the valley's lowest point. She would then need to change direction and step toward the lowest point of the valley again. By taking decreasingly large steps she can avoid stepping back and forth over the valley's lowest point. The blindfolded person continues to walk until she cannot take a step that will decrease her altitude; at that point she has found the bottom of the valley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gradient descent to find the parameters that minimize a real-valued cost function, C, of many variables. Gradient descent iteratively updates the parameters by calculating the partial derivative of the cost function at each step. For this example, we will assume that C is a function of two variables v1 and v2. To minimize C using gradient descent, we need a small change in the variables to produce a small change in the output. Resuming our blind person analogy, we need to be able to take one step in the direction with the steepest descent at a time to reach the valley. We will represent a change in the value of v1 with Δv1, and a change in the value of v2 with Δv2. Taking a small step Δv1 in the v1 direction and Δv2 in the v2 direction results in a small change in the value of C, ΔC. More formally, we can relate the change in C to the changes in v1 and v2 with the following:\n",
    "\n",
    "![Fig](imgs/img_0014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0016.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each iteration, we will calculate the gradient of C, and then subtract η*deba_c from our variables vector to take a step in the direction of the steepest decline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that gradient descent estimates the local minimum of a function. Convex cost functions have a single minimum. A real-valued function is convex if a line segment between any two points on the graph of the function lies above or on the graph. A three-dimensional plot of the values of a convex cost function for all possible values of the parameters looks like a bowl. The bottom of the bowl is the minimum. Conversely, non-convex\n",
    "functions can have many local minimas. The plots of the values of their cost functions can have many peaks and valleys. Gradient descent is only guaranteed to find a local minimum; it will find a valley, but it will not necessarily find the lowest valley. Fortunately, the residual sum of squares cost function is convex.\n",
    "An important hyperparameter of gradient descent is the learning rate, which controls the lengths of the blindfolded person's strides. If the learning rate is small enough, the cost function will decrease with each iteration until gradient descent has converged on the optimal parameters. As the learning rate decreases, however, the time required for gradient descent to converge increases; the blindfolded person will take longer to reach the valley if she takes small steps than if she takes large steps. If the learning rate is too large, she may repeatedly overstep the bottom of the valley; that is, gradient descent could oscillate around the optimal values of the parameters without converging.<br>\n",
    "Three varieties of gradient descent are distinguished by the number of training instances that are used to update the model parameters in each training iteration. Batch gradient descent uses all of the training instances to update the model parameters in each iteration. Stochastic gradient descent, in contrast, updates the parameters using only a single training instance in each iteration. The training instance is usually selected randomly. Both of these variants can be thought of as special cases of mini-batch gradient descent, which uses a batch of b training instances in each iteration.<br>\n",
    "Mini-batch or stochastic gradient descent are often preferred when there are hundreds of thousands of training instances or more, as they will converge more quickly than batch gradient descent. Batch gradient descent is a deterministic algorithm, and will produce the same parameter values given the same training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like gradient descent, backprop is an iterative algorithm. Each iteration consists of two phases. The first phase is forward propagation, or the forward pass. In the forward pass, inputs are propagated through the network's layers of neurons until they reach the output layer. The loss function can then be used to calculate the error of the prediction. The second phase is backward propagation. Errors are propagated backward from the cost function towards the inputs so that each neuron's contribution to the error can be approximated. This process is based on the chain rule, which can be used to calculate the derivative of the composition of two or more functions. We demonstrated earlier that neural networks can compose linear functions together to approximate complex, nonlinear functions. These errors can then be used to calculate the gradients that gradient descent requires to update the weights. After the weights have been updated, features can be propagated forward through the network again to begin the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0018.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector is [0.8, 0.3], and the true value of the response variable is 0.5. Let's calculate the first forward pass, starting with hidden unit h1. First we will calculate the preactivation of h1. Then we will apply the logistic sigmoid function to the preactivation to calculate the activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0020.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = 0.743*0.6 + 0.615*0.2 + 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128749234206665"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "1 /(1 + np.exp(-h2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same process to calculate the activation of h2, which is 0.615. We then input the activations of hidden units h1 and h2 to the output layer and similarly calculate the activation of o1, 0.813. We can now calculate the error of the network's prediction. For this network, we will use the squared error cost function, which is given by the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0021.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, n is the number of output units, ŷi is the activation of output neuron oi, and yi is the true value of the response variable. Our network has a single output unit, so n is 1. The network predicted 0.813; the true value of the response variable is 0.5, so the error is 0.313. Now we can work through updating the weight w5. We must first calculate ∂E/∂w5, or how much changing w5 affects the error. By the chain rule, ∂E/∂w5 is equal to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0022.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then subtract the product of ∂E/∂w5 and our learning rate from the value of w5 to update it. Let's answer the first question by approximating how the error changes with respect to the activation of 01. The partial derivative of the cost function with respect to the activation of the output unit is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_0024.png)\n",
    "![Fig](imgs/img_0025.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv",
   "language": "python",
   "name": "itv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
